<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Lesson 3: Dealing With Data - XSEDE Tutorial</title>
<!-- Bootstrap -->
<link href="assets/css/bootstrap-4.0.0.css" rel="stylesheet">
<link href="assets/css/prism_coy/prism.css" rel="stylesheet" type="text/css">
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-dark bg-dark"> <a class="navbar-brand" href="#">Data Science With Python</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
      <li class="nav-item active"> <a class="nav-link" href="index.html">Home <span class="sr-only">(current)</span></a> </li>
      <li class="nav-item"> <a class="nav-link" href="https://www.xsede.org">XSEDE</a> </li>
      <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> Lessons </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="1.html">Lesson 1</a> <a class="dropdown-item" href="2.html">Lesson 2</a> <a class="dropdown-item" href="3.html">Lesson 3</a> <a class="dropdown-item" href="4.html">Lesson 4</a> <a class="dropdown-item" href="5.html">Lesson 5</a> <a class="dropdown-item" href="6.html">Lesson 6</a> <a class="dropdown-item" href="7.html">Lesson 7</a> <a class="dropdown-item" href="8.html">Lesson 8</a>
          <div class="dropdown-divider"></div>
          <a class="dropdown-item" href="#"></a> </div>
      </li>
    </ul>
  </div>
</nav>
<div class="jumbotron jumbotron-fluid text-center">
  <h1 class="display-4">Data Science with Python</h1>
  <p class="lead">Working with large datasets using Python, Pandas, Matplotlib, and other data science tools.</p>
  <hr class="my-4">
</div>
<div class="container">
  <div class="row text-left">
    <div class="col-lg-12 offset-lg-0">
      <h1>Lesson 3: Dealing with Data</h1>
      <h3>After these lessons you will be able to:      </h3>
      <ul>
        <li>List different forms of data</li>
        <li>Describe the datasets used in these lessons</li>
        <li>List some of the commonly-used tools for accessing, importing and manipulating data            </li>
      </ul>
      <p>For These Lessons You Will Need...</p>
      <ul>
        <li>Python 2.7 or later</li>
        <li>The following packages:</li>
        <li>tweepy </li>
        <li>pandas </li>
        <li>datetime </li>
        <li>json</li>
        <li>(more tbd)</li>
      </ul>
      <p>These lessons are supported by the content at  the following Github repository: </p>
      <p><a href="https://github.com/jsale/data_science_with_python">https://github.com/jsale/data_science_with_python</a></p>
      <p>&nbsp;</p>
      <h2>3.1. Different Forms of Data</h2>
      <p>Data come in many different forms, formats,  sizes, shapes and conduits.¬†  Understanding the structure of data that you will be working with will  facilitate your choice of tools needed to do that work.</p>
      <h3>3.1.1 Regular Data</h3>
      <p>Scientific data are often regular in the structure,  and can be efficiently represented with multidimensional arrays.¬† Common examples include the 3-dimensional  coordinates of every particle in a system (represented, e.g., as an N x 3 array  of floating point numbers), a set of data relating measurements of an  independent and dependent variable (represented as an N x 2 array), or a  continuous field discretized into voxels in a 3-dimensional volume (represented  as an Nx x Ny x Nz array).</p>
      <h3>3.1.2 Heterogeneous Data</h3>
      <p>Like the unhappy families of Tolstoy's famous  quotation, every heterogeneous dataset is heterogeneous in its own way.¬† Some datasets are potentially hierarchical  aggregates of multiple sub-datasets (each of which might more be regular in  structure), which are related to each other in meaningful ways.¬† Some represent data that are defined on an  irregular or complex geometry, such as an unstructured mesh, an underlying  network, or an inhomogeneous sampling of points in space.¬† Some are regularly structured tables of  observations, but where variables being observed are of different type or  structure (see tabular data below for a fuller discussion).¬† As a result, there is no &quot;one size fits  all&quot; data structure for capturing this diversity of structures, although  there are commonly recurring types of structures (hierarchical collections,  meshes, networks, tables, etc.) and associated tools for managing them.¬† Identifying the types of heterogeneity in  data, and the interactions among its elements, is a first step in pinpointing  useful tools for data processing.</p>
      <h3>3.1.3 Tabular Data</h3>
      <p>An important class of heterogeneous data,  alluded to above, are tabular data.¬†  These are two-dimensional datasets of the sort found in spreadsheets,  with a set of rows and columns.¬†  While  nominally regular in structure, tabular data can take on more complex structures. For example, a canonical form of tabular data -- known in  some circles as &quot;tidy&quot; data. There are three interrelated rules which make a dataset tidy:</p>
      <ol>
        <li>Each variable must have its own column.</li>
        <li>Each observation must have its own row.</li>
        <li>Each value must have its own cell.</li>
      </ol>
      <p>We will not go into further detail about 'tidy' data, but you can learn more at ??. </p>
      <p>The simplistic 'unconnected' nature of tabular data is why it is often contrasted with Relational data, which we discuss next.</p>
      <h3>3.1.4 Relational Data</h3>
      <p>Relational data, at least informally here, are  the sort found in relational databases, involving a collection of tables (as in  tabular data, above) that are linked to each other through one or ore shared  keys in the tables.¬† </p>
      <h3>3.1.5 &quot;Big&quot; Data</h3>
      <p>Whether you are working with 'big' data or not is often difficult to determine. Often, issues such as whether you are memory bound or cpu bound impacts such a determination? This means you are working with data that is either too large to load into a reasonable amout of memory or too large to process within a reasonable amount of time on a typical system. In the past, a few gigabytes was considered alot of data. Now, most computers can handle gigabytes of data without too much difficulty. Challenges begin to emerge when working with terabytes or petabytes of data on a daily basis. Simply moving data from one system to another can be a vyer difficult task.      </p>
      <h3>3.1.6 Image Data</h3>
      <p>Image data is becoming more common in data science applications. There are a wide range of image formats, resolutions, color depths to work with. Processing images is often a task for matrix math and linear algebra. Image data is based on a 'raster' format of rows and columns of pixels. Individual pixel color can be 8-bit grayscale or 32-bit RGBA (red, green, blue, alpha). Pixels have a hue, saturation, and intensity measures. Image processing often includes things such as edge detection, de-blurring, pattern recognition, compression, noise reduction, segmentation, differencing,  histograms, and much more.  </p>
      <h3>3.1.7 Textual Data</h3>
      <p>Textual data can be found in many sources, books, news articles, social media, historical archives, etc.. Textual data consists of ascii character strings organized in rows ending with line breaks. Computational linguists work with large text corpi consisting of terabytes of data.      </p>
      <h3>3.1.8 Streaming Data</h3>
      <p>Streaming data is more of a reference to the state in which data is being managed. Streaming data can be text or numeric. Twitter data is an example of streaming data and can be accessed with the Twitter Streaming API. Weather data often streams live from distributed weather sensor networks all over the world. Often, streaming data is so voluminous that it gets processed and filtered before it is saved and the original data is discarded. </p>
      <h3>3.1.9 Hierarchical Data</h3>
      <p>Hierarchical data is distinguished by having a more complex, parent-child, type of relationship between the data. JSON or XML formats are examples. Uncompressed, these text formats use more memory but they offer the advantage of conforming to a format which makes it easy for analytics tools to parse them. </p>
      <p>&nbsp;</p>
      <h2>3.2: Tools for Accessing,  Importing and Manipulating Data</h2>
      <h3>3.2.1 NumPy</h3>
      <p>NumPy is the cornerstone of the Python  scientific computing ecosystem, providing support for multidimensional arrays  (objects of type ndarray) that form the basis of much data representation in  Python. <a href="https://cvw.cac.cornell.edu/python/numpy" target="new">Learn more about Numpy...</a></p>
      <p></p>
      <h3>3.2.2 Pandas</h3>
      <p>Pandas is an extremely powerful library for  manipulating tabular data, of the sort that one typically finds in a  spreadsheet or a record-oriented flat file. </p>
      <p><a href="https://pandas.pydata.org/" target="new">Learn more about Pandas</a>. </p>
      <h3>3.2.3 HDF5</h3>
      <p>[from Wikipedia] The Hierarchical Data Format (HDF) is designed to store and organize large amounts of data. Originally developed at the National Center for Supercomputing Applications, it is supported by The HDF Group, a non-profit corporation whose mission is to ensure continued development of HDF5 technologies and the continued accessibility of data stored in HDF.

HDF is supported by many commercial and non-commercial software platforms, including  Python.</p>
      <p><a href="https://www.hdfgroup.org/solutions/hdf5/" target="new">Learn more about HDF5...</a></p>
      <h3>3.2.4 Dask</h3>
      <p>Dask is a flexible library for parallel computing in Python.

Dask is composed of two parts:

Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.
‚ÄúBig Data‚Äù collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.</p>
      <p><a href="https://docs.dask.org/en/latest/" target="new">Learn more about Dask...</a></p>
      <h3>3.2.5 SQLAlchemy</h3>
      <p>SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. It provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language.      </p>
      <p><a href="https://www.sqlalchemy.org/" target="new">Learn more about SQLAlchemy...</a></p>
      <h3>3.2.6 SQLite3</h3>
      <p>A SQLite database works well in devices that must operate without expert human support because it requires no administration.¬†SQLite is not directly comparable to client/server SQL database engines such as MySQL, Oracle, PostgreSQL, or SQL Server since SQLite is trying to solve a different problem.</p>
      <p>Client/server SQL database engines strive to implement a shared repository of enterprise data. They emphasize scalability, concurrency, centralization, and control. SQLite strives to provide local data storage for individual applications and devices. SQLite emphasizes economy, efficiency, reliability, independence, and simplicity. SQLite does not compete with client/server databases. SQLite competes with fopen().</p>
      <p><a href="https://www.sqlite.org/about.html" target="new">Learn more about SQLite...</a></p>
      <h3>3.2.7 Scikit-image</h3>
      <p>Scikit-image is a collection of algorithms for image processing. </p>
      <p>Learn more about Scikit-image...</p>
      <h3>3.2.8 PIL-Pillow</h3>
      <p>The Python Image Library (PIL), the more recent version (or fork) of which is called &quot;Pillow&quot;, is an image processing library for Python. </p>
      <p><a href="https://pillow.readthedocs.io/en/stable/" target="new">Learn more about Pillow...</a>      </p>
      <p></p>
      <h2>3.3: Introducing Datasets</h2>
      <h3>3.3.1 Baseball Statistical Historical Data</h3>
      <p>Our first set of practical examples  will use the Lahman Baseball Database (LBD). You&nbsp; can learn more about the LBD  and download the data here:</p>
      <p><u><a href="http://www.seanlahman.com/baseball-archive/statistics/">http://www.seanlahman.com/baseball-archive/statistics/</a></u></p>
      <p>Our first step, once we have downloaded the  data, is to identify the location of the sub-directory containing the core  CSV-format files. In this example we put them in a &lsquo;baseballdatabank/core/&rsquo;  folder relative to our scripts. </p>
      <p>To follow along with these lessons, you will  need to import these packages:</p>
      <pre><code class="language-python">import pandas as pd
import glob, os
import seaborn as sns<br>
import matplotlib as mpl<br>
import matplotlib.pyplot as plt<br>
from sqlalchemy import create_engine</code></pre>
      <p>&nbsp;</p>
      <p>Next, we define a function to read all of the  files into a single pandas dataframe, dfs, and then run it on the above folder. </p>
      <pre><code class="language-python">def read_all_databank_core_csv(directory):
    """
    read all csv files in the specified baseball databank directory and
    populate a dictionary storing each of the tables keyed to its name
    """
    dfs = {}
    files = glob.glob('{}/*.csv'.format(directory))
    for f in files:
        d, name = os.path.split(f)
        table = os.path.splitext(name)[0]
        df = pd.read_csv(f)
        dfs[table] = df
    return dfs

bbdfs = read_all_databank_core_csv('baseballdatabank/core')
</code></pre>
      <p>To keep things simple, we will extract three  primary tables from our dataframe, &lsquo;Batting&rsquo;, &lsquo;Pitching&rsquo;, and Teams. </p>
	<pre><code class="language-python"># extract a few for further processing
batting = bbdfs['Batting']
pitching = bbdfs['Pitching']
teams = bbdfs['Teams']</code></pre>
	  <p>&nbsp;</p>
      <h3>3.3.2 Let&rsquo;s Take a Peek</h3>
      <p>Run the following command to view the first 5  rows of data from the &lsquo;Batting&rsquo; table:</p>
      <pre><code class="language-python">batting.head()</code></pre>
      <p>We see that, by default, a Jupyter Notebook  does not display all columns of data with the head() command if there are too  many columns to include is a single cell width. Notice the &lsquo;...&rsquo; between the  &lsquo;2B&rsquo; and &lsquo;SB&rsquo; columns. We can list the columns for one of our tables with  something like:</p>
      <pre><code class="language-python">pitching.columns</code></pre>
      <h3>3.3.3 Adding Derived Data</h3>
      <p>One of the interesting limits to the LBD  Batting table is the lack of a specific statistic for Single Base Hits. This  can be derived by summing the Doubles (2B), Triples (3B), and Home Runs (HR)  and subtracting this sum from the total number of Hits. </p>
      <pre><code class="language-python">batting['1B'] = batting['H'] - batting['2B'] -  batting['3B'] - batting['HR']</code></pre>
      <p>This may also be performed for the &lsquo;Teams&rsquo;  table.      </p>
      <p>In Lesson 4, Statistics with Data, we perform some analysis on this dataset using pandas, and in Lesson 5, we create some examples of how to visualize the data to gain additional insight into underlying patterns. </p>
      <h2>3.3.4 Twitter</h2>
      <p>You are probably already familiar with the  Twitter social network. Twitter is a network built on a messaging system which  allows people to send and receive messages consisting of fewer than 280  characters (and attached images) and other limited types of metadata. It has  been used effectively as a near real-time communications tool in numerous  global social events. Perhaps its greatest strength is that Twitter also  provides an API which allows anyone to collect large amounts of data and  perform a wide range of analyses to better understand these networks. This strength  can also be a weakness, introducing vulnerabilities to &lsquo;bots&rsquo; or &lsquo;state-backed&rsquo;  accounts which are used to spread disinformation in critical ways. </p>
      <p>In these lessons, you will learn how to use the  Twitter Search API and the Streaming API to collect tweets using the Python  Tweepy library. Then, in Lesson 5, you will perform graph network analysis using networkx, and you  will learn how to export to other graph network analysis and visualization  applications such as Gephi, Neo4j, and GraphX. </p>
      <p>Before you can start collecting Twitter data,  you will need to create a developer account, get authenticated, and create a  Twitter application. Here is a brief description of the process with links to  more information. </p>
      <h3>3.3.5 Get Authenticated For the Twitter API</h3>
      <p><a href="https://developer.twitter.com/en/docs/basics/authentication/guides/access-tokens.html">https://developer.twitter.com/en/docs/basics/authentication/guides/access-tokens.html</a></p>
      <h3><br>
        3.3.6 Create a Twitter Application</h3>
      <p><a href="https://developer.twitter.com/en/docs/basics/apps">https://developer.twitter.com/en/docs/basics/apps</a></p>
      <p>Once you have created your application and have  your authentication keys and tokens, you will need to install the tweepy  package in Python. The command you use depends on how you have configured your  Python environment. If you are using Anaconda and prefer to install packages  with conda, use something like the following command: </p>
      <pre><code class="language-bash">conda install -c conda-forge tweepy </code></pre>
      </p>
      <p>If you are just using straight Python, use the  following command: </p>
      <pre><code class="language-bash">pip install tweepy </code></pre>
      <p>A similar process applies for any other package  you might find you need. Learn more about Python packages in the <a href="https://cvw.cac.cornell.edu/pythonintro/">CVW Intro  to Python tutorial</a>.</p>
      <h3><br>
        3.3.7 Using the Twitter Streaming API</h3>
      <p>Most users of XSEDE systems already have  collected their Twitter data and simply want to analyze it, but we will walk  through a simple example of how to collect streaming tweet data that is  collected using a filter, such as containing a specific hashtag. </p>
      <p>Here is a simple example of how to collect  tweets with hashtag #nerd, print the tweet data and save it to a file. </p>
      <p><a href="https://drive.google.com/open?id=1WpcYPs-yXUOAEqeLjgJaAgzVLiXIlrs9">https://drive.google.com/open?id=1WpcYPs-yXUOAEqeLjgJaAgzVLiXIlrs9</a></p>
      <p><pre><code class="language-python"># -*- coding: utf-8 -*-
"""
Created on Sat Oct 27 08:59:27 2018

@author: XSEDE
"""

import re
import datetime
from __future__ import absolute_import, print_function

from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream

# Go to http://apps.twitter.com and create an app.
# The consumer key and secret will be generated for you after
consumer_key="your_consumer_key"
consumer_secret="your_consumer_secret"

# After the step above, you will be redirected to your app's page.
# Create an access token under the the "Your access token" section
access_token="your_access_token"
access_token_secret="your_access_token_secret"

# Create variables to use for output file and tweet filter
hashtag = "maga"
date_time_temp = str(datetime.datetime.now())

# Replace all characters except letters and numbers with "_" for filename
current_date_time = re.sub('[^a-zA-Z0-9]','_', date_time_temp)
file_out = open(hashtag + "_" + current_date_time + ".json", 'a')

# Define the Stream Listener
class StdOutListener(StreamListener):
    """ A listener handles tweets that are received from the stream.
    This is a basic listener that just prints received tweets to stdout.

    """
    def on_data(self, data):
        print(data) # Print output to console
        file_out.write(data) # Write output to file
        return True

    def on_error(self, status):
        print(status)

# Run the main program and collect tweets with hashtag "nerd"
if __name__ == '__main__':
    l = StdOutListener()
    auth = OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)

    stream = Stream(auth, l)
    stream.filter(track=['nerd'])
file_out.close()</code></pre></p>
      <h3>3.3.8 Simplify the Dataset By Converting JSON to CSV and Extracting Subset of Parameters</h3>
      <p>The standard format for results returned from a  Twitter API search is JSON (JavaScript Object Notation). For large amounts of Twitter data it can be useful to reduce the size of the data by extracting a subset and re-saving it in a CSV format. We will use the Python csv and json  packages to convert the data from JSON to CSV. To get an idea how much data is in a single tweet, below is an example of a tweet in JSON format: </p>
      <p><a href="https://drive.google.com/open?id=1IReYbFmqzI4ZsOInvVeIwSUKT3k0NERM7MChBdmOe6k">https://drive.google.com/open?id=1IReYbFmqzI4ZsOInvVeIwSUKT3k0NERM7MChBdmOe6k</a></p><pre><code class="language-json">{'contributors': None,
 'coordinates': None,
 'created_at': 'Sat Jul 14 17:02:02 +0000 2018',
 'entities': {'hashtags': [{'indices': [77, 88], 'text': 'motoz3play'},
                           {'indices': [112, 124], 'text': 'electronics'},
                           {'indices': [125, 133], 'text': 'glasses'},
                           {'indices': [134, 139], 'text': 'geek'}],
              'symbols': [],
              'urls': [],
              'user_mentions': [{'id': 824881,
                                 'id_str': '824881',
                                 'indices': [3, 11],
                                 'name': "Luisito '#ImBack' GyG üá≤üáΩ‚öΩ",
                                 'screen_name': 'LuisGyG'},
                                {'id': 4883803745,
                                 'id_str': '4883803745',
                                 'indices': [64, 76],
                                 'name': 'Motorola de M√©xico',
                                 'screen_name': 'Motorola_MX'}]},
 'favorite_count': 0,
 'favorited': False,
 'geo': None,
 'id': 1018178822087192581,
 'id_str': '1018178822087192581',
 'in_reply_to_screen_name': None,
 'in_reply_to_status_id': None,
 'in_reply_to_status_id_str': None,
 'in_reply_to_user_id': None,
 'in_reply_to_user_id_str': None,
 'is_quote_status': False,
 'lang': 'es',
 'metadata': {'iso_language_code': 'es', 'result_type': 'recent'},
 'place': None,
 'retweet_count': 1,
 'retweeted': False,
 'retweeted_status': {'contributors': None,
                      'coordinates': None,
                      'created_at': 'Sat Jul 14 16:40:41 +0000 2018',
                      'entities': {'hashtags': [{'indices': [64, 75],
                                                 'text': 'motoz3play'},
                                                {'indices': [99, 111],
                                                 'text': 'electronics'}],
                                   'symbols': [],
                                   'urls': [{'display_url': 'twitter.com/i/web/status/1‚Ä¶',
                                             'expanded_url': 'https://twitter.com/i/web/status/1018173447581720576',
                                             'indices': [113, 136],
                                             'url': 'https://t.co/8jqvWUPc5c'}],
                                   'user_mentions': [{'id': 4883803745,
                                                      'id_str': '4883803745',
                                                      'indices': [51, 63],
                                                      'name': 'Motorola de '
                                                              'M√©xico',
                                                      'screen_name': 'Motorola_MX'}]},
                      'favorite_count': 5,
                      'favorited': False,
                      'geo': None,
                      'id': 1018173447581720576,
                      'id_str': '1018173447581720576',
                      'in_reply_to_screen_name': None,
                      'in_reply_to_status_id': None,
                      'in_reply_to_status_id_str': None,
                      'in_reply_to_user_id': None,
                      'in_reply_to_user_id_str': None,
                      'is_quote_status': False,
                      'lang': 'es',
                      'metadata': {'iso_language_code': 'es',
                                   'result_type': 'recent'},
                      'place': None,
                      'possibly_sensitive': False,
                      'retweet_count': 1,
                      'retweeted': False,
                      'source': '<a href="https://ifttt.com" '
                                'rel="nofollow">IFTTT</a>',
                      'text': 'Hoy toca switch de tel√©fono y me estar√© mudando '
                              'al @motorola_mx #motoz3play Les cuento. ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ '
                              '‚Ä¢ \n'
                              '#electronics‚Ä¶ https://t.co/8jqvWUPc5c',
                      'truncated': True,
                      'user': {'contributors_enabled': False,
                               'created_at': 'Fri Mar 09 05:40:17 +0000 2007',
                               'default_profile': False,
                               'default_profile_image': False,
                               'description': 'El Fan√°tico de los Gadgets. '
                                              'üì∫üì±üì∑üîäüéÆ\n'
                                              'Conferencista sobre '
                                              'tendencias.\n'
                                              'Blogger. \n'
                                              'Contrataciones y contacto: '
                                              'contacto@luisgyg.com.',
                               'entities': {'description': {'urls': []},
                                            'url': {'urls': [{'display_url': 'luisgyg.com',
                                                              'expanded_url': 'http://www.luisgyg.com',
                                                              'indices': [0,
                                                                          23],
                                                              'url': 'https://t.co/ZgFJDzRhkD'}]}},
                               'favourites_count': 3989,
                               'follow_request_sent': False,
                               'followers_count': 64080,
                               'following': False,
                               'friends_count': 1107,
                               'geo_enabled': True,
                               'has_extended_profile': True,
                               'id': 824881,
                               'id_str': '824881',
                               'is_translation_enabled': False,
                               'is_translator': False,
                               'lang': 'es',
                               'listed_count': 1515,
                               'location': 'M√©xico',
                               'name': "Luisito '#ImBack' GyG üá≤üáΩ‚öΩ",
                               'notifications': False,
                               'profile_background_color': '131516',
                               'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme14/bg.gif',
                               'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme14/bg.gif',
                               'profile_background_tile': True,
                               'profile_banner_url': 'https://pbs.twimg.com/profile_banners/824881/1530210899',
                               'profile_image_url': 'http://pbs.twimg.com/profile_images/1012403930997780481/foIo_8sD_normal.jpg',
                               'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1012403930997780481/foIo_8sD_normal.jpg',
                               'profile_link_color': 'FF0000',
                               'profile_sidebar_border_color': 'FFFFFF',
                               'profile_sidebar_fill_color': 'EFEFEF',
                               'profile_text_color': '333333',
                               'profile_use_background_image': True,
                               'protected': False,
                               'screen_name': 'LuisGyG',
                               'statuses_count': 92668,
                               'time_zone': None,
                               'translator_type': 'regular',
                               'url': 'https://t.co/ZgFJDzRhkD',
                               'utc_offset': None,
                               'verified': True}},
 'source': '<a href="http://twitter.com/download/android" '
           'rel="nofollow">Twitter for Android</a>',
 'text': 'RT @LuisGyG: Hoy toca switch de tel√©fono y me estar√© mudando al '
         '@motorola_mx #motoz3play Les cuento. ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ \n'
         '#electronics #glasses #geek‚Ä¶',
 'truncated': False,
 'user': {'contributors_enabled': False,
          'created_at': 'Mon Jan 02 22:18:36 +0000 2012',
          'default_profile': True,
          'default_profile_image': False,
          'description': 'Simpre corriendo con el coraz√≥n x delante,  el '
                         'coraje empujando, llegando a la meta hasta el ultimo '
                         'aliento, la satisfacci√≥n de llegar. Preparandome '
                         'ultra trail',
          'entities': {'description': {'urls': []}},
          'favourites_count': 1034,
          'follow_request_sent': False,
          'followers_count': 48,
          'following': False,
          'friends_count': 229,
          'geo_enabled': False,
          'has_extended_profile': True,
          'id': 453415076,
          'id_str': '453415076',
          'is_translation_enabled': False,
          'is_translator': False,
          'lang': 'es',
          'listed_count': 0,
          'location': 'Nicol√°s Romero, M√©xico',
          'name': 'Hunter Sephiroth',
          'notifications': False,
          'profile_background_color': 'C0DEED',
          'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',
          'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',
          'profile_background_tile': False,
          'profile_banner_url': 'https://pbs.twimg.com/profile_banners/453415076/1505333131',
          'profile_image_url': 'http://pbs.twimg.com/profile_images/983326797113823232/e57SjsdS_normal.jpg',
          'profile_image_url_https': 'https://pbs.twimg.com/profile_images/983326797113823232/e57SjsdS_normal.jpg',
          'profile_link_color': '1DA1F2',
          'profile_sidebar_border_color': 'C0DEED',
          'profile_sidebar_fill_color': 'DDEEF6',
          'profile_text_color': '333333',
          'profile_use_background_image': True,
          'protected': False,
          'screen_name': 'Huntersephiroth',
          'statuses_count': 819,
          'time_zone': None,
          'translator_type': 'none',
          'url': None,
          'utc_offset': None,
          'verified': False}}

</code></pre></p>
      <p>All tweets have this type of content in general,  but the details may vary depending on various things, such as whether it is an  original tweet or a retweet. Retweets include a &lsquo;retweeted_status&rsquo; parameter  which contains all of the information about the original tweet which was  retweeted. Other differences may be whether the tweet is geolocated with a  latitude and longitude of the user&rsquo;s location, and so on.      </p>
      <p>The dataset we will be working with was created using the Twitter Search API and searching on the hashtag 'nerd'. Tweets were collected every 15 minutes and saved to a file. After two weeks the files were processed to remove duplicate tweets and combined into a single file. Duplicate tweets are an artifact of requesting the maximum number of tweets for each 15 minute epoch. Twitter limits the Search API to 100 tweets per 15 minute epoch. They post 150 in their documentation but we have observed it to be 100.</p>
      <p>In this example, we reduce the dataset from ~70 Mb down to ~5 Mb, and we will save two files, one containing basic tweet parameters and the other containing tweet text. This is probably not necessary for such a small dataset but it becomes useful when working with much larger datasets consisting of millions of tweets.</p>
      <p>The parameters we choose to retain in the CSV include the following, which is by no means comprehensive:</p>
      <ul>
        <li>tweet_id</li>
        <li>tweet_created_at</li>
        <li>language</li>
        <li>user_screen_name</li>
        <li>user_created_at</li>
        <li>user_id</li>
        <li>followers_count</li>
        <li>friends_count</li>
        <li>time_zone</li>
        <li>utc_offset</li>
        <li>retweeted_status</li>
        <li>retweet_id</li>
        <li>retweet_user_screen_name</li>
        <li>retweet_user_id</li>
      </ul>
      <p>Import Packages</p>
      <pre><code class="language-python"># Load packages
import os
import csv
import sys
import json
import datetime
from pprint import pprint</code></pre>
	<p>Now we read the data in and re-save it as a CSV format file. </p>
	<pre><code class="language-python"># Print start time at start and end time at end
print("Start:" + str(datetime.datetime.now()))

# Open CSV output files for writing
output_dir = "csv/"
hashtag = "nerd"

# Open main twitter data CSV file and write header row
main_output_file = output_dir + hashtag + "_main.csv"
f_main = open(main_output_file, 'w', newline='')
mainrowwriter = csv.writer(f_main, delimiter=',')
main_outputstring = ['tweet_id','tweet_created_at','language','user_screen_name','user_created_at','user_id','followers_count','friends_count','time_zone','utc_offset','retweeted_status','retweet_id','retweet_user_screen_name','retweet_user_id']
mainrowwriter.writerow(main_outputstring)

# Open twitter text data CSV file and write header row
text_output_file = output_dir + hashtag + "_text.csv"
f_text = open(text_output_file, 'w', errors='ignore', newline='')
textrowwriter = csv.writer(f_text, delimiter=',')
text_outputstring = ['tweet_id','text']
textrowwriter.writerow(text_outputstring)

# Define variables
inc = 0
val = 0
val_inc = 0
dir = 'tweet_data/'
filename = 'nerd.json'

with open(dir + filename, 'r') as f:
    print("Working on file:" + filename)
    data = json.load(f)
    for tweet in data:
        if 'user' in tweet:
            
            # Set standard variables equal to tweet data
            tweet_id = tweet['id']
            tweet_created_at = tweet['created_at']
            text = tweet['text']
            language = tweet['lang']
            user_screen_name = tweet['user']['screen_name']
            user_created_at = tweet['user']['created_at']
            user_id = tweet['user']['id']
            followers_count = tweet['user']['followers_count']
            friends_count = tweet['user']['friends_count']
            utc_offset = tweet['user']['utc_offset']
            time_zone = tweet['user']['time_zone']
            
            # Check if a retweet else original tweet
            if 'retweeted_status' in tweet:
                retweeted_status = 1
                retweet_id = tweet['retweeted_status']['id']
                retweet_user_screen_name = tweet['retweeted_status']['user']['screen_name']
                retweet_user_id = tweet['retweeted_status']['user']['id']
            else:
                retweeted_status = 0
                retweet_id = "None"
                retweet_user_screen_name = "None"
                retweet_user_id = "None"
            
            # Write to main output file
            main_outputstring = [str(tweet_id), tweet_created_at, language, user_screen_name, user_created_at, str(user_id), str(followers_count), str(friends_count), time_zone, utc_offset, str(retweeted_status), str(retweet_id), retweet_user_screen_name, str(retweet_user_id)] 
            mainrowwriter.writerow(main_outputstring)
            
            # Write to text output file
            text_outputstring = [str(tweet_id), text]
            textrowwriter.writerow(text_outputstring)
            
            # Increment variables to track progress, mostly for very large files
            inc += 1
            val_inc += 1
            if val_inc > 10000:
                val = val + 10000
                print(str(val))
                val_inc = 0

# Close all files
f.close()
f_main.close()
f_text.close()
print("End:" + str(datetime.datetime.now()))</code></pre>
	  <h2>3.3.?? Others?</h2>
    </div>
  </div>
  <br>
  <hr>
  <div class="row">
    <div class="text-center col-lg-6 offset-lg-3">
      <h4>--</h4>
      <p>Copyright &copy; 2019 XSEDE</p>
    </div>
  </div>
</div>
<script type="text/javascript" src="assets/js/prism_coy/prism.js"></script> 
<!-- jQuery (necessary for Bootstrap's JavaScript plugins) --> 
<script src="assets/js/jquery-3.2.1.min.js"></script> 

<!-- Include all compiled plugins (below), or include individual files as needed --> 
<script src="assets/js/popper.min.js"></script> 
<script src="assets/js/bootstrap-4.0.0.js"></script>
</body>
</html>
